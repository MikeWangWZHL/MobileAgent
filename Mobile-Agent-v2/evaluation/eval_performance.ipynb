{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import copy\n",
    "\n",
    "class TaskPerformanceEvaluator:\n",
    "    def __init__(self, rubrics):\n",
    "        self.rubrics = {\n",
    "            i: rubrics[i] for i in range(len(rubrics))\n",
    "        }\n",
    "        self.rubrics_satisfaction = {\n",
    "            i: False for i in range(len(rubrics))\n",
    "        }\n",
    "        self.rubrics_satisfaction_history = [] # satisfaction per action step\n",
    "        self.action_count = 0\n",
    "        self.correct_action_count = 0\n",
    "        self.reflection_count = 0\n",
    "        self.correct_reflection_count = 0\n",
    "\n",
    "        self.error_recovery_score = None\n",
    "        \n",
    "        self.early_termination = None\n",
    "        self.early_termination_reason = None\n",
    "    \n",
    "    def update_rubrics_satisfaction(self):\n",
    "        print(\"### Rubric Status ###\", flush=True)\n",
    "        for i, rubric in self.rubrics.items():\n",
    "            emoji = \"✅\" if self.rubrics_satisfaction[i] else \"❌\"\n",
    "            print(f\"{i}: {rubric} | Satisfied: {self.rubrics_satisfaction[i]} {emoji}\", flush=True)\n",
    "        print(\"\\n\")\n",
    "        while True:\n",
    "            res = input(f\"Any new rubrics are satisfied? If yes Enter the index or indices seperated by comma of the rubrics. Otherwise, enter 'n'.\")\n",
    "            \n",
    "            # no new rubrics are satisfied\n",
    "            if res.strip().lower() == 'n' or res.strip().lower() == \"\":\n",
    "                print(\"No change to rubric status.\", flush=True)\n",
    "                self.rubrics_satisfaction_history.append(copy.deepcopy(self.rubrics_satisfaction))\n",
    "                break\n",
    "\n",
    "            # new rubrics are satisfied\n",
    "            indices = res.split(\",\")\n",
    "            for ind in indices:\n",
    "                if ind in [str(i) for i in range(len(self.rubrics))]:\n",
    "                    self.rubrics_satisfaction[int(ind)] = True\n",
    "                else:\n",
    "                    print(f\"Invalid input \\\"{res}\\\". Please enter the index or indices of the rubrics.\", flush=True)\n",
    "                    continue\n",
    "            print(\"### Updated Rubric Status ###\", flush=True)\n",
    "            for i, rubric in self.rubrics.items():\n",
    "                emoji = \"✅\" if self.rubrics_satisfaction[i] else \"❌\"\n",
    "                print(f\"{i}: {rubric} | Satisfied: {self.rubrics_satisfaction[i]} {emoji}\", flush=True)\n",
    "            print(\"\\n\")\n",
    "            self.rubrics_satisfaction_history.append(copy.deepcopy(self.rubrics_satisfaction))\n",
    "            break\n",
    "\n",
    "    def update_action_count(self):\n",
    "        while True:\n",
    "            res = input(\"Action is correct? Enter 'y' for yes, 'n' for no\")\n",
    "            if res.strip().lower() == 'y':\n",
    "                self.action_count += 1\n",
    "                self.correct_action_count += 1\n",
    "                break\n",
    "            elif res.strip().lower() == 'n':\n",
    "                self.action_count += 1\n",
    "                break\n",
    "            elif res.strip().lower() == 'stop':\n",
    "                raise SystemExit(\"Terminated by user.\")\n",
    "            else:\n",
    "                print(\"Invalid input. Please enter 'y' or 'n'.\")\n",
    "\n",
    "    def update_reflection_count(self):\n",
    "        while True:\n",
    "            res = input(\"Reflection is correct? Enter 'y' for yes, 'n' for no\")\n",
    "            if res.strip().lower() == 'y':\n",
    "                self.reflection_count += 1\n",
    "                self.correct_reflection_count += 1\n",
    "                break\n",
    "            elif res.strip().lower() == 'n':\n",
    "                self.reflection_count += 1\n",
    "                break\n",
    "            elif res.strip().lower() == 'stop':\n",
    "                raise SystemExit(\"Terminated by user.\")\n",
    "            else:\n",
    "                print(\"Invalid input. Please enter 'y' or 'n'.\")\n",
    "\n",
    "    def compute_error_recovery(self, steps):\n",
    "        prev_is_error = False\n",
    "        errors = 0\n",
    "        recovered_errors = 0\n",
    "        for i, step in enumerate(steps):\n",
    "            if 'operation' in step and step['operation'] == 'action_reflection':\n",
    "                outcome = step[\"outcome\"][:2]\n",
    "                if \"B\" in outcome or \"C\" in outcome:\n",
    "                    errors += 1\n",
    "                    prev_is_error = True\n",
    "                else:\n",
    "                    if prev_is_error:\n",
    "                        recovered_errors += 1\n",
    "                    prev_is_error = False\n",
    "        if errors != 0:\n",
    "            self.error_recovery_score = recovered_errors / errors\n",
    "\n",
    "    def check_early_termination(self, steps):\n",
    "        last_step = steps[-1]\n",
    "        if 'operation' in last_step and last_step['operation'] == 'finish':\n",
    "            if last_step['finish_flag'] == \"max_iteration\":\n",
    "                self.early_termination = True\n",
    "                self.early_termination_reason = \"max_iteration\"\n",
    "            elif last_step['finish_flag'] == \"max_consecutive_failures\":\n",
    "                self.early_termination = True\n",
    "                self.early_termination_reason = \"max_consecutive_failures\"\n",
    "            elif last_step['finish_flag'] == \"max_repetitive_actions\":\n",
    "                self.early_termination = True\n",
    "                self.early_termination_reason = \"max_repetitive_actions\"\n",
    "            \n",
    "\n",
    "    def save_evaluation(self, output_path, steps = None):\n",
    "        if steps is not None:\n",
    "            if len(steps) != 0:\n",
    "                self.compute_error_recovery(steps)\n",
    "                self.check_early_termination(steps)\n",
    "        if self.action_count == 0:\n",
    "            print(\"No action decision made. Evaluation not saved.\")\n",
    "            return\n",
    "        output = {\n",
    "            \"rubrics\": self.rubrics,\n",
    "            \"rubrics_satisfaction\": self.rubrics_satisfaction,\n",
    "            \"rubrics_satisfaction_history\": self.rubrics_satisfaction_history,\n",
    "            \"action_count\": self.action_count,\n",
    "            \"correct_action_count\": self.correct_action_count,\n",
    "            \"reflection_count\": self.reflection_count,\n",
    "            \"correct_reflection_count\": self.correct_reflection_count,\n",
    "            \"action_accuracy\": self.correct_action_count / self.action_count,\n",
    "            \"reflection_accuracy\": self.correct_reflection_count / self.reflection_count,\n",
    "            \"perfectly_done\": all(self.rubrics_satisfaction.values()),\n",
    "            \"satisfactory_score\": sum(self.rubrics_satisfaction.values()) / len(self.rubrics_satisfaction),\n",
    "            \"error_recovery_score\": self.error_recovery_score,\n",
    "            \"early_termination\": self.early_termination,\n",
    "            \"early_termination_reason\": self.early_termination_reason\n",
    "        }\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(output, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def show_pre_cur_screenshot(cur, prev=None):\n",
    "    if prev is not None:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax[0].imshow(prev)\n",
    "        ax[0].set_title(\"Previous Screenshot\")\n",
    "        ax[0].axis('off')\n",
    "        ax[1].imshow(cur)\n",
    "        ax[1].set_title(\"Current Screenshot\")\n",
    "        ax[1].axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(cur)\n",
    "        plt.title(\"Current Screenshot\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "def eval_loop_on_task(log_dir, all_rubrics):\n",
    "    output_eval_json = os.path.join(log_dir, \"evaluation.json\")\n",
    "    if os.path.exists(output_eval_json):\n",
    "        print(\"INFO: evaluation already exists in\", output_eval_json)\n",
    "        return\n",
    "\n",
    "\n",
    "    steps = json.load(open(os.path.join(log_dir, \"steps.json\")))\n",
    "    task_id = steps[0]['task_id']\n",
    "    rubrics = all_rubrics[task_id]['rubrics']\n",
    "    screenshot_dir = os.path.join(log_dir, \"screenshots\")\n",
    "    evaluator = TaskPerformanceEvaluator(rubrics=rubrics)\n",
    "    print(\"Rubrics:\", evaluator.rubrics)\n",
    "    task = steps[0]['instruction']\n",
    "    ### start eval loop\n",
    "    prev_screen_shot = None\n",
    "    current_screen_shot = None\n",
    "    current_action = None\n",
    "    current_action_thought = None\n",
    "    current_action_description = None\n",
    "    current_reflection = None\n",
    "    current_reflection_thought = None\n",
    "    current_note = None\n",
    "    print(\"==========================================\\n\")\n",
    "    for si, step in enumerate(steps):\n",
    "        # current_progress = None\n",
    "        if step['operation'] == \"perception\":\n",
    "            screenshot_basename = os.path.basename(step['screenshot'])\n",
    "            if current_screen_shot is None:\n",
    "                current_screen_shot = Image.open(os.path.join(screenshot_dir, screenshot_basename))\n",
    "                # show_pre_cur_screenshot(current_screen_shot, prev=prev_screen_shot)\n",
    "            else:\n",
    "                prev_screen_shot = current_screen_shot\n",
    "                current_screen_shot = Image.open(os.path.join(screenshot_dir, screenshot_basename))\n",
    "                show_pre_cur_screenshot(current_screen_shot, prev=prev_screen_shot)\n",
    "\n",
    "        if step['operation'] == \"notetaking\":\n",
    "            current_note = step['important_notes']\n",
    "            print(f\"%%% Current Note %%%: {current_note}\", flush=True)\n",
    "            print(\"-----\", flush=True)\n",
    "\n",
    "        if step['operation'] == \"action\":\n",
    "            current_action = step['action_object']\n",
    "            current_action_thought = step['action_thought']\n",
    "            # current_action_description = step['description']\n",
    "            print(f\"%%% Current Action %%%: {current_action}\", flush=True)\n",
    "            print(f\"%%% Current Action Thought %%%: {current_action_thought}\", flush=True)\n",
    "            print(\"-----\", flush=True)\n",
    "\n",
    "        if step['operation'] == \"action_reflection\":\n",
    "            print(\"Task:\", task, flush=True)\n",
    "            print(\"\\n\")\n",
    "            current_reflection = step['outcome']\n",
    "            # current_reflection_thought = step['raw_response'].split(\"### Answer ###\")[0].strip()\n",
    "            print(f\"%%% Current Action Reflection %%%: {current_reflection}\", flush=True)\n",
    "            # print(f\"%%% Current Action Reflection Thoughts %%%:\", {current_reflection_thought}, flush=True)\n",
    "            print(\"\\n\")\n",
    "            print(\"*** Eval action...\")\n",
    "            evaluator.update_action_count()\n",
    "            print(\"*** Eval action reflection...\", flush=True)\n",
    "            evaluator.update_reflection_count()\n",
    "            print(\"*** Update rubric satisfaction...\", flush=True)\n",
    "            evaluator.update_rubrics_satisfaction()\n",
    "            print(\"==========================================\\n\")\n",
    "            sleep(1)\n",
    "            # Clear output at the beginning of each iteration\n",
    "            clear_output(wait=True)\n",
    "            plt.close('all')\n",
    "            plt.pause(0.1)\n",
    "        \n",
    "        if step['operation'] == \"finish\" or si == len(steps) - 1:\n",
    "            print(f\"%%% final check on rubrics...\", flush=True)\n",
    "            show_pre_cur_screenshot(current_screen_shot, prev=prev_screen_shot)\n",
    "            evaluator.update_rubrics_satisfaction()\n",
    "            print(\"###########################################\\n\")\n",
    "            sleep(1)\n",
    "            # Clear output at the beginning of each iteration\n",
    "            clear_output(wait=True)\n",
    "            plt.close('all')\n",
    "            plt.pause(0.1)\n",
    "\n",
    "        \n",
    "    evaluator.save_evaluation(output_eval_json, steps=steps)\n",
    "    print(\"INFO: evaluation saved to\", output_eval_json, flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: evaluation saved to /Users/wangz3/Desktop/vlm_agent_project/MobileAgent/Mobile-Agent-v2/logs/mobile_agent_v2/scenario_5_batch_v1.json-individual/5_things_to_do_la/evaluation.json\n"
     ]
    }
   ],
   "source": [
    "### all rubrics batch v1\n",
    "all_rubrics = json.load(open(\"/Users/wangz3/Desktop/vlm_agent_project/MobileAgent/Mobile-Agent-v2/data/batch_v1/rubrics/batch_v1_rubrics.json\"))\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "### Mobile Agent v2 Eval ###\n",
    "# scenario_dir = \"/Users/wangz3/Desktop/vlm_agent_project/MobileAgent/Mobile-Agent-v2/logs/mobile_agent_v2/scenario_1_batch_v1.json-individual\" # done\n",
    "# scenario_dir = \"/Users/wangz3/Desktop/vlm_agent_project/MobileAgent/Mobile-Agent-v2/logs/mobile_agent_v2/scenario_2_batch_v1.json-individual\" # done\n",
    "# scenario_dir = \"/Users/wangz3/Desktop/vlm_agent_project/MobileAgent/Mobile-Agent-v2/logs/mobile_agent_v2/scenario_3_batch_v1.json-individual\" # done\n",
    "# scenario_dir = \"/Users/wangz3/Desktop/vlm_agent_project/MobileAgent/Mobile-Agent-v2/logs/mobile_agent_v2/scenario_4_batch_v1.json-individual\" # done\n",
    "scenario_dir = \"/Users/wangz3/Desktop/vlm_agent_project/MobileAgent/Mobile-Agent-v2/logs/mobile_agent_v2/scenario_5_batch_v1.json-individual\" # done\n",
    "log_dirs = sorted(glob(os.path.join(scenario_dir, \"*\")))\n",
    "for log_dir in log_dirs:\n",
    "    print(\"Evaluating:\", log_dir)\n",
    "    eval_loop_on_task(log_dir, all_rubrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: evaluation saved to /Users/wangz3/Desktop/vlm_agent_project/MobileAgent/Mobile-Agent-v2/logs/agent_E/scenario_5_batch_v1.json-individual/5_things_to_do_la/evaluation.json\n"
     ]
    }
   ],
   "source": [
    "### all rubrics batch v1\n",
    "all_rubrics = json.load(open(\"/Users/wangz3/Desktop/vlm_agent_project/MobileAgent/Mobile-Agent-v2/data/batch_v1/rubrics/batch_v1_rubrics.json\"))\n",
    "\n",
    "from glob import glob\n",
    "### Agent E Eval ###\n",
    "\n",
    "# scenario_dir = \"/Users/wangz3/Desktop/vlm_agent_project/MobileAgent/Mobile-Agent-v2/logs/agent_E/scenario_1_batch_v1.json-individual\"\n",
    "# scenario_dir = \"/Users/wangz3/Desktop/vlm_agent_project/MobileAgent/Mobile-Agent-v2/logs/agent_E/scenario_2_batch_v1.json-individual\"\n",
    "# scenario_dir = \"/Users/wangz3/Desktop/vlm_agent_project/MobileAgent/Mobile-Agent-v2/logs/agent_E/scenario_3_batch_v1.json-individual\"\n",
    "# scenario_dir = \"/Users/wangz3/Desktop/vlm_agent_project/MobileAgent/Mobile-Agent-v2/logs/agent_E/scenario_4_batch_v1.json-individual\"\n",
    "# scenario_dir = \"/Users/wangz3/Desktop/vlm_agent_project/MobileAgent/Mobile-Agent-v2/logs/agent_E/scenario_5_batch_v1.json-individual\"\n",
    "\n",
    "log_dirs = sorted(glob(os.path.join(scenario_dir, \"*\")))\n",
    "for log_dir in log_dirs:\n",
    "    print(\"Evaluating:\", log_dir)\n",
    "    eval_loop_on_task(log_dir, all_rubrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
